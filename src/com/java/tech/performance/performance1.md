# ðŸ“˜ Performance Testing Interview Q&A Guide

This document covers:
1. **Performance Testing Interview Questions & Answers**
2. **Core Knowledge Areas for Performance Test Engineers**

---

## ðŸ”¹ Basics

**Q: What is performance testing and how is it different from functional testing?**  
âœ… Performance testing checks **speed, scalability, stability, and responsiveness** of an application under load.
- **Functional testing** â†’ verifies correctness of features.
- **Performance testing** â†’ verifies how fast, stable, and scalable features work under expected/peak load.

---

**Q: Explain the types of performance tests.**  
âœ…
- **Load Test** â†’ system under expected user load.
- **Stress Test** â†’ push beyond limits to find breaking point.
- **Soak/Endurance Test** â†’ run under load for long duration (memory leaks, stability).
- **Spike Test** â†’ sudden surge in traffic (e.g., flash sale).
- **Scalability Test** â†’ check behavior when scaling infra.
- **Volume Test** â†’ DB performance with large datasets.

---

**Q: What are key performance testing KPIs?**  
âœ…
- **Response Time** (avg, 95th/99th percentile).
- **Latency** (network delay).
- **Throughput / TPS**.
- **Error Rate** (% failed requests).
- **Resource Utilization** (CPU, memory, disk I/O, network).

---

## ðŸ”¹ Tools & Frameworks

**Q: Which tools have you worked on?**  
âœ… JMeter, LoadRunner, Locust, Gatling, Vegeta, k6.

---

**Q: How do you parameterize test data?**  
âœ… Use CSV files, env variables, or generators. (e.g., JMeter CSV DataSet Config, Locust Python generators).

---

**Q: How do you simulate concurrent users vs requests per second?**  
âœ…
- **Concurrent Users** â†’ simulate multiple virtual users (threads, coroutines).
- **RPS (Requests per Second)** â†’ control request rate explicitly (good for APIs).

---

**Q: How do you handle correlation & dynamic values?**  
âœ… Extract dynamic values (session IDs, tokens) from server response and reuse.
- JMeter â†’ Regex/JSON Extractor.
- LoadRunner â†’ Correlation rules.

---

## ðŸ”¹ Scenario Design

**Q: How do you design a workload model?**  
âœ… Use production traffic patterns. Example:
- 60% login, 30% checkout, 10% search.
- Include ramp-up, steady-state, ramp-down.

---

**Q: Whatâ€™s the difference between think time and pacing?**  
âœ…
- **Think Time** â†’ simulated user delay.
- **Pacing** â†’ controls rate of execution per user.

---

**Q: How do you decide number of users to simulate?**  
âœ… Formula:  Concurrent Users = (Peak Transactions per Hour Ã— Avg Response Time in seconds) / 3600


---

**Q: How do you distribute load across endpoints?**  
âœ… Weighted distribution (e.g., 60% login, 30% search, 10% checkout).
- JMeter â†’ Throughput Controller.
- Locust â†’ Task weights.

---

## ðŸ”¹ Metrics & Monitoring

**Q: How do you measure response time vs latency?**  
âœ…
- **Latency** â†’ time to first byte.
- **Response Time** â†’ full round-trip until response complete.

---

**Q: Difference between throughput and bandwidth?**  
âœ…
- **Throughput** â†’ requests/sec handled.
- **Bandwidth** â†’ network link capacity.

---

**Q: How do you monitor infra during tests?**  
âœ… APM tools (Grafana, Prometheus, New Relic).
- DB monitoring: slow query logs.
- Infra: `top`, `vmstat`, `iostat`.

---

**Q: How do you identify bottlenecks?**  
âœ…
- High CPU + slow responses â†’ CPU bottleneck.
- Low CPU + slow responses â†’ DB/network issue.
- Memory grows continuously â†’ memory leak.

---

## ðŸ”¹ Analysis & Results

**Q: What are 95th/99th percentile response times?**  
âœ… Metrics showing worst-case user experience.
- 95th percentile = 95% of requests are faster than this time.

---

**Q: How do you analyze performance results?**  
âœ… Compare vs baseline & SLA. Look at response time trends, throughput, and errors.

---

**Q: How do you compare test runs?**  
âœ… Keep workload/env same, compare throughput, response times, error %.

---

**Q: How do you decide pass/fail?**  
âœ… Based on SLA. Example:
- 95% requests < 2 sec.
- Error rate < 1%.
- CPU < 70%.

---

## ðŸ”¹ Advanced

**Q: How do you test microservices performance?**  
âœ… Run API-level tests, measure inter-service latency, DB queries. Use service mesh telemetry.

---

**Q: How do you test async systems (Kafka, RabbitMQ)?**  
âœ… Measure producer throughput, consumer lag, message latency.

---

**Q: How do you test real-time systems (chat/streaming)?**  
âœ… Use WebSocket clients (Locust, k6). Measure message latency & delivery rate.

---

**Q: How do you integrate perf tests into CI/CD?**  
âœ… Run JMeter/Locust in Jenkins, GitHub Actions. Fail build if SLA thresholds breached.

---

**Q: How do you handle distributed load?**  
âœ… JMeter distributed mode, Locust workers, Kubernetes injectors.

---

## ðŸ”¹ Troubleshooting

**Q: CPU high but response time good?**  
âœ… Likely CPU-bound but still optimized. Watch if it hits 100%.

---

**Q: Response time bad but CPU low?**  
âœ… Likely DB bottleneck, query tuning, or network issue.

---

**Q: How do you detect memory leaks?**  
âœ… Run endurance test â†’ monitor memory. Continuous growth â†’ leak.

---

**Q: Throughput drops when users increase?**  
âœ… Resource saturation, thread contention, DB locks.

---

# ðŸ“˜ 2. Core Knowledge Areas

### âœ… Performance Testing Types
- Load, Stress, Soak, Spike, Scalability, Volume.

### âœ… Metrics
- Response time (avg, p95/p99), throughput, TPS, error %, resource usage.

### âœ… Tools
- Load tools: JMeter, Locust, Gatling, k6.
- Monitoring: Grafana, Prometheus, New Relic.
- Profiling: Heap/thread dumps, DB analysis.

### âœ… Systems Knowledge
- HTTP, REST, WebSockets.
- Caching (Redis, CDN).
- SQL/NoSQL performance.
- Cloud scaling, load balancers.

### âœ… Skills to Demonstrate
1. Convert SLAs â†’ performance goals.
2. Create scalable scripts.
3. End-to-end monitoring.
4. Identify bottlenecks.
5. Recommend optimizations.
6. Work with dev/ops to fix issues.

---

# ðŸŽ¯ Key Takeaway
Be ready to:
- Explain **types of performance tests**.
- Design **realistic workloads**.
- Monitor **full-stack performance**.
- Analyze & provide **actionable insights**.
- Collaborate on optimizations.  


```
Requests      [total, rate]            6000, 200.00
Duration      [total, attack, wait]    30.0s, 29.9s, 100ms
Latencies     [mean, 95, 99, max]      320ms, 480ms, 600ms, 1s
Bytes In      [total, mean]            1200000, 200.00
Success       [ratio]                  99.5%
Status Codes  [code:count]             200:5970  500:30

1. Requests [total, rate] 6000, 200.00
Total Requests = 6000 â†’ During the test, Vegeta sent 6000 HTTP requests.
Rate = 200 RPS â†’ Each second, 200 requests were sent (200 Ã— 30s = 6000).
ðŸ‘‰ This shows your system was attacked with a steady load of 200 requests/sec for 30 seconds.

2. Duration [total, attack, wait] 30.0s, 29.9s, 100ms
Total = 30.0s â†’ Total runtime of test.
Attack = 29.9s â†’ Time Vegeta was actively sending requests.
Wait = 100ms â†’ Extra time for outstanding requests to complete.

3. Latencies
Latencies [mean, 95, 99, max] 320ms, 480ms, 600ms, 1s

Mean (Avg) = 320 ms â†’ Average response time across all 6000 requests.
95th Percentile (p95) = 480 ms â†’
95% of requests completed within 480 ms.
Only 5% of requests took longer.
99th Percentile (p99) = 600 ms â†’
99% of requests completed within 600 ms.
Only 1% of requests were slower.
Max = 1s â†’ The single slowest request took 1 second.

Why percentiles matter?
Average can be misleading (hides slow outliers).
Percentiles (p95, p99) show tail latency â€” critical for user experience.
Example: if you have 1M users, 1% (10k users) seeing 600ms response is still a lot.

4. Bytes In : 
Bytes In [total, mean] 1200000, 200.00
Total = 1,200,000 bytes â†’ Total data received from server (~1.2 MB).
Mean = 200 bytes per response â†’ Each response body averaged 200 bytes.

5. Success :
Success [ratio] 99.5%
99.5% of requests succeeded.
This means 0.5% (â‰ˆ30 requests) failed (likely returned error codes).

6. Status codes : 
Status Codes [code:count] 200:5970  500:30
200 (OK) â†’ 5970 requests succeeded.
500 (Internal Server Error) â†’ 30 requests failed.

```
